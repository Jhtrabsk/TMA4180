import numpy as np
from numpy.linalg import inv
from numpy import linalg as LA


def f(X):
    x, y = X 
    return 100*(y-x**2)**2 + (1-x)**2

def grad_f(X):
    x, y = X 
    f_x = -400*x*(y-x**2) - 2*(1-x)
    f_y =  200*(y-x**2)
    return np.array([f_x,f_y])

def H_f(X): 
    x, y = X 
    f_xx = -400*y + 1200*x**2 + 2
    f_yy = -400*x
    f_xy =  200
    return LA.inv(np.array([[f_xx,f_xy],
                           [f_xy,f_yy]]))
    

class BFGS:    
#    
   def __init__(this, B_k_1, x_k_1, iterations): 
        this.B_k_1 = B_k_1
        this.x_k_1 = x_k_1
        this.iterations = iterations
#          
   def update_step_direvtion(this, grad_f, B_k, x_k): 
#  
#   Update step derivation
#   written by Jan Haakon melka Trabski 2023
#   This function return the next step in the BFGS
#  
    alpha = 1
#       
    p_k = -B_k@grad_f(x_k)
#
#    This makes it worse?    
#    alpha =  line_search(f, grad_f, x_k, p_k)                     
#
    s_k = alpha*p_k 
#
    this.x_k_1 = x_k + s_k
    y_k = grad_f(this.x_k_1) - grad_f(x_k)
#
    y_k = np.reshape(y_k,(2,1))
    s_k = np.reshape(s_k,(2,1))
#
    r = 1/(y_k.T@s_k)
    li = (np.eye(2)-(r*((s_k@(y_k.T)))))
    ri = (np.eye(2)-(r*((y_k@(s_k.T)))))
#
    this.B_k_1 = li@B_k@ri + (r*((s_k@(s_k.T))))
    
    this.iterations += 1
#    
#

def print_summary(grad_f, B_k_1, x_k_1, iteration, converged):
    
    if(converged): 
        f = open("Summary.txt", "a")
        f.write("the geometry has converged")
        f.close()
        
    else: 
        f = open("Summary.txt", "a")
        f.write("current iteration %s" % iteration + "\n")
        f.write("current position: %s " % x_k_1+ "\n")
        f.write("current gard %s" % grad_f+ "\n")
        f.write("Hessian: \n %s " % B_k_1 + "\n")
        f.write("\n")
    

def bfgs_solver(grad_f, gradient_threshold, H_0, x_0):
#
    iterations = 0
    converged = False
#
    bfgs = BFGS(H_0, x_0, iterations)
    bfgs.update_step_direvtion(grad_f, bfgs.B_k_1, bfgs.x_k_1)
    print_summary(grad_f(bfgs.x_k_1), bfgs.B_k_1, bfgs.x_k_1, bfgs.iterations, converged)
#
    while LA.norm(grad_f(bfgs.x_k_1)) > gradient_threshold:
#
        bfgs = BFGS(bfgs.B_k_1, bfgs.x_k_1, bfgs.iterations)
        bfgs.update_step_direvtion(grad_f, bfgs.B_k_1, bfgs.x_k_1)  
#
        print_summary(grad_f(bfgs.x_k_1), bfgs.B_k_1, bfgs.x_k_1, bfgs.iterations, converged)
        
    if(LA.norm(grad_f(bfgs.x_k_1)) < gradient_threshold):
        converged = True 
        print_summary(grad_f(bfgs.x_k_1), bfgs.B_k_1, bfgs.x_k_1, bfgs.iterations, converged)
#    
bfgs_solver(grad_f, 1e-5, np.eye(2),np.array([10,10]))
