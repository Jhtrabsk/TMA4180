import numpy as np
import matplotlib.pyplot as plt 
from numpy.linalg import inv
from numpy import linalg as LA


def f(X):
    x, y = X 
    return 100*(y-x**2)**2 + (1-x)**2

def grad_f(X):
    x, y = X 
    f_x = -400*x*(y-x**2) - 2*(1-x)
    f_y =  200*(y-x**2)
    return np.array([f_x,f_y])

def H_f(X): 
    x, y = X 
    f_xx = -400*y + 1200*x**2 + 2
    f_yy = -400*x
    f_xy =  200
    return LA.inv(np.array([[f_xx,f_xy],
                           [f_xy,f_yy]]))
    

class BFGS:    
#    
    def __init__(this, B_k_1, x_k_1, iterations, converged): 
        this.B_k_1 = B_k_1
        this.x_k_1 = x_k_1
        this.iterations = iterations
        this.converged = converged
#          
    def update_step_direvtion(this, grad_f, B_k, x_k): 
#  
#   Update step derivation
#   written by Jan Haakon melka Trabski 2023
#   This function return the next step in the BFGS
#  
        alpha = 1.0
#       
        p_k = -B_k@grad_f(x_k)
#
#    This makes it worse?    
#    alpha =  line_search(f, grad_f, x_k, p_k)                     
#
        s_k = alpha*p_k 
#
        this.x_k_1 = x_k + s_k
        y_k = grad_f(this.x_k_1) - grad_f(x_k)
#
        y_k = np.reshape(y_k,(2,1))
        s_k = np.reshape(s_k,(2,1))
#
        r = 1/(y_k.T@s_k)
        li = (np.eye(2)-(r*((s_k@(y_k.T)))))
        ri = (np.eye(2)-(r*((y_k@(s_k.T)))))
#
        this.B_k_1 = li@B_k@ri + (r*((s_k@(s_k.T))))
#    
        this.iterations += 1
#
    def print_summary(this):
#  
#   print
#   written by Jan Haakon Melka, Trabski 2023
#   Prints out the summary of geoopt
#
        if(this.iterations == 1):
           open("Summary.txt", "w").close()           
#
        if(this.converged): 
#
            f = open("Summary.txt", "a")
            f.write("the geometry has converged")
            f.close()
#    
        else: 
#
            f = open("Summary.txt", "a")
            f.write("current iteration %s" % this.iterations + "\n")
            f.write("current position: %s " % this.x_k_1+ "\n")
            f.write("current gard %s" % grad_f(this.x_k_1) + "\n")
            f.write("Hessian: \n %s " % this.B_k_1 + "\n")
            f.write("\n")
    
    def plot(x_store): 
       
        x1 = np.linspace(min(x_store[:,0]-0.5),max(x_store[:,0]+0.5),30)
        x2 = np.linspace(min(x_store[:,1]-0.5),max(x_store[:,1]+0.5),30)
        X1,X2 = np.meshgrid(x1,x2)
        Z = f([X1,X2])
        plt.figure()
        plt.title('OPTIMAL AT: '+str(x_store[-1,:])+'\n IN '+str(len(x_store))+' ITERATIONS')
        plt.contourf(X1,X2,Z,30,cmap='jet')
        plt.colorbar()
        plt.plot(x_store[:,0],x_store[:,1],c='w')
        plt.xlabel('$x_1$'); plt.ylabel('$x_2$')
        plt.show()


#            
#
# ---------------------------------------------------            
#
# 
  
def bfgs_solver(grad_f, gradient_threshold, H_0, x_0):
#
#
    iterations = 0
    converged = False
#
    bfgs = BFGS(H_0, x_0, iterations, converged)
    bfgs.update_step_direvtion(grad_f, bfgs.B_k_1, bfgs.x_k_1)
    bfgs.print_summary()
#
    while LA.norm(grad_f(bfgs.x_k_1)) > gradient_threshold:
#
        bfgs = BFGS(bfgs.B_k_1, bfgs.x_k_1, bfgs.iterations, bfgs.converged)
        bfgs.update_step_direvtion(grad_f, bfgs.B_k_1, bfgs.x_k_1)
        bfgs.print_summary()

#
    if(LA.norm(grad_f(bfgs.x_k_1)) < gradient_threshold):
        bfgs = BFGS(bfgs.B_k_1, bfgs.x_k_1, bfgs.iterations, True)
        bfgs.print_summary()
        


bfgs_solver(grad_f, 1e-5, np.eye(2),np.array([-100,100]))
